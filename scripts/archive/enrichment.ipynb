{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "140278b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Namespace, URIRef, BNode, Literal\n",
    "from rdflib.namespace import RDF, RDFS, OWL\n",
    "import argparse, pathlib, textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "da53fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "OIO       = Namespace(\"http://www.geneontology.org/formats/oboInOwl#\")\n",
    "IAO_DEF   = URIRef(\"http://purl.obolibrary.org/obo/IAO_0000115\")\n",
    "HAS_SYN = OIO.hasSynonym\n",
    "HAS_XREF  = OIO.hasDbXref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2bc5c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graft_axiom(g: Graph, subj: URIRef, prop: URIRef,\n",
    "                tgt_literal: Literal, xref_literal: Literal):\n",
    "    \"\"\"\n",
    "    Add:\n",
    "        subj  prop  tgt_literal .\n",
    "    and:\n",
    "        [ rdf:type         owl:Axiom ;\n",
    "          owl:annotatedSource    subj ;\n",
    "          owl:annotatedProperty  prop ;\n",
    "          owl:annotatedTarget    tgt_literal ;\n",
    "          oio:hasDbXref          xref_literal ] .\n",
    "    \"\"\"\n",
    "    # assertion triple\n",
    "    g.add((subj, prop, tgt_literal))\n",
    "\n",
    "    # reified axiom\n",
    "    ax = BNode()\n",
    "    g.add((ax, RDF.type,          OWL.Axiom)) # add axiom node\n",
    "    g.add((ax, OWL.annotatedSource,   subj)) # \n",
    "    g.add((ax, OWL.annotatedProperty, prop))\n",
    "    g.add((ax, OWL.annotatedTarget,   tgt_literal))\n",
    "    g.add((ax, HAS_XREF,              xref_literal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5b33fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Main conversion routine\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def migrate(in_path: pathlib.Path, out_path: pathlib.Path):\n",
    "    g = Graph()\n",
    "    g.parse(in_path)                        # auto-detect format\n",
    "\n",
    "    for cls in g.subjects(RDFS.seeAlso, None):\n",
    "        # ---- 1) NCIT ID -----------------------------------------------------\n",
    "        ncit_raw = str(next(g.objects(cls, RDFS.seeAlso)))\n",
    "        ncit_id  = ncit_raw if ncit_raw.startswith(\"NCIT:\") else f\"NCIT:{ncit_raw}\"\n",
    "        xref_lit = Literal(ncit_id)\n",
    "\n",
    "        # ---- 2) Split comments into name vs definition ----------------------\n",
    "        coms = list(g.objects(cls, RDFS.comment))\n",
    "        if len(coms) < 2:\n",
    "            continue        # skip if we don't have both pieces\n",
    "        coms_sorted = sorted(coms, key=lambda lit: len(str(lit))) \n",
    "        name_lit, def_lit = coms_sorted[0], coms_sorted[-1] # definition is usually longer than name\n",
    "\n",
    "        # ---- 3) Add new annotations wrapped in owl:Axiom --------------------\n",
    "        graft_axiom(g, cls, IAO_DEF,   def_lit,  xref_lit)\n",
    "        graft_axiom(g, cls, HAS_SYN, name_lit, xref_lit)\n",
    "\n",
    "        # ---- 4) (optional) cleanup ------------------------------------------\n",
    "        g.remove((cls, RDFS.comment, name_lit))\n",
    "        g.remove((cls, RDFS.comment, def_lit))\n",
    "        g.remove((cls, RDFS.seeAlso, Literal(ncit_raw)))\n",
    "\n",
    "    g.serialize(out_path, format=\"xml\")     # default RDF/XML\n",
    "    print(f\"✔  Saved enriched ontology to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bcaee3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_FILE = '_hpvco.rdf'\n",
    "DST_FILE = 'hpvco.rdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c68abf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔  Saved enriched ontology to hpvco.rdf\n"
     ]
    }
   ],
   "source": [
    "migrate(pathlib.Path(SRC_FILE), pathlib.Path(DST_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b8a4c2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file structure:\n",
      "              Domain    ID                                             SPARQL   \n",
      "0  General Knowledge  CQ_1  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-sc...  \\\n",
      "1                NaN  CQ_2  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...   \n",
      "2                NaN  CQ_3  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-sc...   \n",
      "3                NaN  CQ_4  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-sc...   \n",
      "4                NaN  CQ_5  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-sc...   \n",
      "\n",
      "                                  Coverage Questions   \n",
      "0         What cancers are related to HPV infection?  \\\n",
      "1  What cancers can be potentially be prevented b...   \n",
      "2  What are the known risk factors  causally rela...   \n",
      "3  What anatomical sites are associated with HPV ...   \n",
      "4                     What does HPV infection cause?   \n",
      "\n",
      "                                     Expected Result   \n",
      "0  \"cervical cancer\"\\n\"penile cancer\"\\n\"anal canc...  \\\n",
      "1  \"cervical cancer\"\\n\"penile cancer\"\\n\"anal canc...   \n",
      "2  \"smoking\"\\n\"alcoholism\"\\n\"high risk sexual beh...   \n",
      "3  \"cervix\"\\n\"penis\"\\n\"anus\"\\n\"vagina\"\\n\"vulva\"\\n...   \n",
      "4  \"hpv related psychological stress\"\\n\"warts\"\\n\"...   \n",
      "\n",
      "                                       Actual Result  \n",
      "0  \"cervical cancer\"\\n\"penile cancer\"\\n\"anal canc...  \n",
      "1  \"cervical cancer\"\\n\"penile cancer\"\\n\"anal canc...  \n",
      "2  \"smoking\"\\n\"alcoholism\"\\n\"high risk sexual beh...  \n",
      "3  \"cervix\"\\n\"penis\"\\n\"anus\"\\n\"vagina\"\\n\"vulva\"\\n...  \n",
      "4  \"hpv related psychological stress\"\\n\"cervical ...  \n",
      "\n",
      "Column names:\n",
      "['Domain', 'ID', 'SPARQL', 'Coverage Questions', 'Expected Result', 'Actual Result']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read the Excel file\n",
    "excel_file = 'CQ_Eval.xlsx'\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Display the structure of the file\n",
    "print(\"Excel file structure:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f7efbf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (20, 6)\n",
      "\n",
      "First 5 rows of ID and SPARQL columns:\n",
      "ID column: 'ID'\n",
      "SPARQL column: 'SPARQL'\n",
      "\n",
      "Data preview:\n",
      "     ID                                             SPARQL\n",
      "0  CQ_1  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-sc...\n",
      "1  CQ_2  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...\n",
      "2  CQ_3  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-sc...\n",
      "3  CQ_4  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-sc...\n",
      "4  CQ_5  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-sc...\n"
     ]
    }
   ],
   "source": [
    "# Let's look at just the first few rows and specific columns\n",
    "print(\"Shape of dataframe:\", df.shape)\n",
    "print(\"\\nFirst 5 rows of ID and SPARQL columns:\")\n",
    "if len(df.columns) >= 3:\n",
    "    # Assuming ID is 2nd column (index 1) and SPARQL is 3rd column (index 2)\n",
    "    id_col = df.columns[1]\n",
    "    sparql_col = df.columns[2]\n",
    "    print(f\"ID column: '{id_col}'\")\n",
    "    print(f\"SPARQL column: '{sparql_col}'\")\n",
    "    print(\"\\nData preview:\")\n",
    "    print(df[[id_col, sparql_col]].head())\n",
    "else:\n",
    "    print(\"Available columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2aa0d200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Created evaluation/competency_questions/CQ_1.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_2.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_3.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_4.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_5.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_6.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_7.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_8.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_9.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_10.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_11.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_12.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_13.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_14.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_15.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_16.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_17.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_18.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_19.sparql\n",
      "✔ Created evaluation/competency_questions/CQ_20.sparql\n",
      "\n",
      "✔ Successfully generated 20 SPARQL query files in evaluation/competency_questions\n"
     ]
    }
   ],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "output_dir = 'evaluation/competency_questions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate individual SPARQL files\n",
    "for index, row in df.iterrows():\n",
    "    if pd.notna(row['ID']) and pd.notna(row['SPARQL']):\n",
    "        query_id = row['ID']\n",
    "        sparql_query = row['SPARQL']\n",
    "        \n",
    "        # Create filename\n",
    "        filename = f\"{query_id}.sparql\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Write SPARQL query to file\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(sparql_query)\n",
    "        \n",
    "        print(f\"✔ Created {filepath}\")\n",
    "\n",
    "print(f\"\\n✔ Successfully generated {len(df)} SPARQL query files in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6577b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_FILE = 'hpvco.owl'\n",
    "DST_FILE = 'hpvco_1.owl'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
